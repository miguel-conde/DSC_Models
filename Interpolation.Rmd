---
title: "Interpolation"
author: "Miguel Conde"
date: "Thursday, April 09, 2015"
output: html_document
---

This documents aims to document and explain the interpolation method implemented.

Till now, predictions are made with a simple backoff algorithm:

- We have a model with 4 tables (1,2,3 and 4 Grams). Each N-Gram has an asociated conditional probability. For example, the probability associated to the 4-gram <WG1, WG2, WG3, WG4> is the probability to find the token WG4 once the trigram <WG1, WG2,WG3> has appeared.
- When trying to predict the next word following  the trigram <WG1, WG2, WG3>, the algorithm first searches the 4-Gram Table. If one or more matches are found, we restringe the possible next word to one of the WG4s found. One is selected according to the associated conditonal probabilities (or sampled, if more than one word have the same maximum conditional probability). 
- If no 3gram matches are found in the 4gram table, we back off to the 3gram table and repeat the process just with a bigram (<WG2,WG3>). An so on.

Now we wanna use interpolation. We begin as before, searching the 4gram table. Suppose we have the trigram <WG1, WG2, WG3> and some matches are found in the 4gram table: <WG1,WG2,WG3,X>, <WG1,WG2,WG3,Y> and <WG1,WG2,WG3,Z>. Now we'll calculate the probabilities as

P(X) = l4*P(X/<WG1,WG2,WG3>) + l3*P(X/<WG2, WG3>) + l2*P(X/WG2) + l1*P(X)

We have to look for P(X/<WG1,WG2,WG3>) in the conditional probabilities of the 4gram table, P(X/<WG2, WG3>) in the trigrams table, P(X/WG2) in the bigrams table and P(X) in the unigrams table.

And need to give values to l1, l2, l3 and l4.

To do that, we'll optimize them in order to minimize perplexity in an statistically significant sample of the text.

This is the code used to accomplish this task:

```{r code_R, message=FALSE, warning=FALSE}

source("./R/perplex.R")
library(DEoptim)

# This function samples Blogs, News and Twitter files to build a Total sample
# with nLines
sampleTotal <- function(nLines) {

  numLines <- ceiling(nLines/3)
  
  linesBlogs   <- sampleLines(blogsSampleTrainFile_1G, numLines)
  linesNews    <- sampleLines(newsSampleTrainFile_1G, numLines)
  linesTwitter <- sampleLines(twitterSampleTrainFile_1G, numLines)
  
  paste0(linesBlogs, linesNews, linesTwitter)
}

# This function samples nLines from file 'inFile'
sampleLines <- function(inFile, nLines) {
  if(inFile == "TOTAL") {
    lines <- sampleTotal(nLines)
  }
  else {
    fileLines <- readLines(inFile, encoding = "UTF-8", skipNul = TRUE)
    idxLines <- sample(length(fileLines), nLines)
    lines <- NULL
    sapply(fileLines[idxLines], function(x) {
      lines <<- paste0(lines, x)
    })
  }
  lines
}

# This is the lambda-minimization function. Lambdas are passed as 
# lamdas/sum(lambdas) to ensure they will sum up to 1 when optimized
# l -> lambdas vector
# model -> which model we'll use to optimize lambdas
# NGProbs -> 4 grams grobability matrix. Each row corresponds to a 4-gram in the
# text. 1st column contains the probability of the associated 1-gram, 2nd 
# column contains the probability of the associated 2-bigram, 3rd column the
# probability of the associated 3-gram and 4th column the probability of the 
# 4-gram.
fun2 <- function(l, model, NGProbs) {
  
  sum_l <- sum(l)
  
  getPerplexity(model, NGProbs, l/sum_l, log = TRUE)
}

# This function makes all the work
# loadModelFun -> function to load the model
# sampleFile -> where to find the sample file from which to read text lines
# nLinesSamples -> number of lines in sampleFile to use for optimizing lambdas
#
# For optimization, package 'DEopim' is used
getLambdas <- function(loadModelFun, sampleFile, nLinesSample) {
  
  # Load model
  trainModel <- loadModelFun()
  
  # Set up data tables keys for work with perplexity
  setkey(trainModel$m.1G, WG1)
  setkey(trainModel$m.2G, WG1, WG2)
  setkey(trainModel$m.3G, WG1, WG2, WG3)
  setkey(trainModel$m.4G, WG1, WG2, WG3, WG4)
  
  # Read text lines 
  lines <- sampleLines(sampleFile, nLinesSample)
  # Transform them into 4-grams
  N4Grams <- get4Grams(lines, trainModel)
  # Get 1,2,3 and 4-gram probabilities for each 4-gram
  N4Probs <- getProbs(trainModel, N4Grams)
  
  # Launch optimization process
  DE <- DEoptim(fn = fun2, lower = rep(0, 4), upper = rep(1, 4), 
                control = DEoptim.control(trace = FALSE),
                model = trainModel, NGProbs = N4Probs)
  
  # Get and return optimized lambdas
  best_lambdas <- DE$optim$bestmem/sum(DE$optim$bestmem)
  best_lambdas
}

```

The functions used in the process above are the following ones:

```{r aux_functions}
# Test function to check perplexity is  computed right
linePerplex4G_v3 <- function(model, line, l = c(0.25, 0.25, 0.25, 0.25)) {
  
  # Get 4-Grams
  NGrams <- get4Grams(model, line)
  
  # Get probs table
  NGProbs <- getProbs <- function(model, NGrams)
  
  # Calc perplexity
  return(getPerplexity(model, NGProbs, l))
  
}

# Function to extract 4-grams from 'line'. 'model' is used to check if a token
# is or not in our dictionary
get4Grams <- function(line, model) {
  # Process punctuation
  s <- managePunct(line)
  
  # tolower
  s <- tolower(s)
  
  # stripwhitespaces
  s <- stripWhitespace(s)
  
  # Add seudowords
  s <- paste0("<stx> <stx> <stx> ", grep("^.*$", s, value = TRUE), " <etx> <etx> <etx>")
  
  # Tokenize
  NGrams <- NGramTokenizer(s, Weka_control(min = 4, max = 4,
                                           delimiters = ' \r\n\t"'))
  
  # Split into tokens and reemplace OOV words by "<unk>"
  NGrams <- strsplit(NGrams, " ")
  NGrams <- as.data.table(t(as.matrix(as.data.frame(NGrams))))
  setkey(NGrams, V1, V2, V3, V4)
  NGrams[V1!="<stx>" & V1!="<etx>" & 
           is.na(model[["m.1G"]][V1]$Count)]$V1 <- "<unk>"
  NGrams[V2!="<stx>" & V2!="<etx>" & 
           is.na(model[["m.1G"]][V2]$Count)]$V2 <- "<unk>"
  NGrams[V3!="<stx>" & V3!="<etx>" & 
           is.na(model[["m.1G"]][V3]$Count)]$V3 <- "<unk>"
  NGrams[V4!="<stx>" & V4!="<etx>" & 
           is.na(model[["m.1G"]][V4]$Count)]$V4 <- "<unk>"
  
  return(NGrams[V4!="<etx>"])
}

# This function builds the probability table
getProbs <- function(model, NGrams) {
  
  #lProb <- NULL
  #sapply(NGrams, function(x) {
  #  probPredict4G_v2(model, x, log = FALSE)
  #})

  NGrams$P1 <- model[["m.1G"]][NGrams[,V4]]$Freq
  NGrams[is.na(NGrams$P1)]$P1 <- 0

  setkey(model[["m.2G"]], WG1, WG2)
  NGrams$P2 <- model[["m.2G"]][.(NGrams$V3, NGrams$V4)]$Freq
  NGrams[is.na(NGrams$P2)]$P2 <- 0
  
  setkey(model[["m.3G"]], WG1, WG2, WG3)
  NGrams$P3 <- model[["m.3G"]][.(NGrams$V2, NGrams$V3, NGrams$V4)]$Freq
  NGrams[is.na(NGrams$P3)]$P3 <- 0
  
  setkey(model[["m.4G"]], WG1, WG2, WG3, WG4)
  NGrams$P4 <- model[["m.4G"]][.(NGrams$V1, NGrams$V2, NGrams$V3, NGrams$V4)]$Freq
  NGrams[is.na(NGrams$P4)]$P4 <- 0
  
  
  return(NGrams)
}

getPerplexity <- function(model, NGProbs, 
                          l = c(0.25, 0.25, 0.25, 0.25), log = FALSE) {
  
  nNGrams <- dim(NGProbs)[2]
  
  # Calc perplexity
  perplexity <- sum(log2(as.matrix(NGProbs[,.(P1,P2,P3,P4)]) %*%  l))
 
  perplexity <- (-1/nNGrams)*perplexity
  
  if(log == FALSE)
    perplexity <- 2^perplexity
  
  return(perplexity)
}

```

Now, we just have to run it:

```{r execute, message=FALSE, warning=FALSE}
init_date <- date()
nLines <- 1200

set.seed(1234)

## BLOGS
best_lambdas_Blogs <- getLambdas(loadModelBlogs, blogsSampleTrainFile_1G, 
                                 nLines)
best_lambdas_Blogs

## NEWS
best_lambdas_News <- getLambdas(loadModelNews, newsSampleTrainFile_1G, 
                                nLines)
best_lambdas_News

## TWITTER
best_lambdas_Twitter <- getLambdas(loadModelTwitter, twitterSampleTrainFile_1G, 
                                   nLines)
best_lambdas_Twitter

## TOTAL
best_lambdas_Total <- getLambdas(loadModelTotal, "TOTAL", nLines)
best_lambdas_Total

res_table <- data.frame(Par     = c("lambda1", "lambda2", "lambda3", "lambda4"),
                        Blogs   = best_lambdas_Blogs,
                        News    = best_lambdas_News,
                        Twitter = best_lambdas_Twitter,
                        Total   = best_lambdas_Total)
res_table
end_date <- date()
print(sprintf("From %s to %s", init_date, end_date))
```

